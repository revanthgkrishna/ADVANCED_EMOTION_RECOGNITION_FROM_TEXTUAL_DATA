{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca4aa6ba-8892-46bf-920f-a1341ef4926f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\project\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\project\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\project\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\project\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\project\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: tensorflow in c:\\project\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: transformers in c:\\project\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: tf-keras in c:\\project\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\project\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\project\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\project\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\project\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\project\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\project\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\project\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\project\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\project\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\project\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\project\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\project\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\project\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\project\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\project\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\project\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\project\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\project\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\project\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\project\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\project\\lib\\site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\project\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\project\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\project\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\project\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\project\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\project\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\project\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\project\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\project\\lib\\site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\project\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\project\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: filelock in c:\\project\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\project\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\project\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\project\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\project\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\project\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\project\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\project\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\project\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: rich in c:\\project\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\project\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\project\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\project\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\project\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\project\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\project\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\project\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\project\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\project\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\project\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\project\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\project\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\project\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\project\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: pyarrow in c:\\project\\lib\\site-packages (16.1.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\project\\lib\\site-packages (from pyarrow) (1.26.4)\n",
      "WARNING:tensorflow:From C:\\Users\\Bharath Esambadi\\AppData\\Local\\Temp\\ipykernel_10016\\3746136092.py:5: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas matplotlib seaborn scikit-learn tensorflow transformers tf-keras\n",
    "!pip install pyarrow\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.losses.sparse_softmax_cross_entropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc5aabf5-cd4a-4439-9881-2300fff1952d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "file_paths = {\n",
    "    \"amazon_polarity_train\": \"C:/Project/dataset_cache/amazon_polarity_train.parquet\",\n",
    "    \"amazon_polarity_test\": \"C:/Project/dataset_cache/amazon_polarity_test.parquet\",\n",
    "    \"go_emotions_train\": \"C:/Project/dataset_cache/go_emotions_train.parquet\",\n",
    "    \"go_emotions_test\": \"C:/Project/dataset_cache/go_emotions_test.parquet\",\n",
    "    \"go_emotions_validation\": \"C:/Project/dataset_cache/go_emotions_validation.parquet\",\n",
    "    \"imdb_train\": \"C:/Project/dataset_cache/imdb_train.parquet\",\n",
    "    \"imdb_test\": \"C:/Project/dataset_cache/imdb_test.parquet\",\n",
    "    \"imdb_unsupervised\": \"C:/Project/dataset_cache/imdb_unsupervised.parquet\"\n",
    "}\n",
    "\n",
    "# Loading datasets into Pandas DataFrames\n",
    "dataframes = {name: pd.read_parquet(path) for name, path in file_paths.items()}\n",
    "print(\"Datasets loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c05b07f2-3870-47f5-9609-c8159a82b1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:/Project/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:/Project/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:/Project/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:/Project/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NLTK resources installed to: C:/Project/nltk_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "# Setting project folder as the NLTK data path\n",
    "project_folder = \"C:/Project/nltk_data\"\n",
    "os.makedirs(project_folder, exist_ok=True)\n",
    "nltk.data.path.append(project_folder)\n",
    "\n",
    "# Download required datasets directly to the project folder\n",
    "nltk.download('punkt', download_dir=project_folder)\n",
    "nltk.download('stopwords', download_dir=project_folder)\n",
    "nltk.download('wordnet', download_dir=project_folder)\n",
    "nltk.download('omw-1.4', download_dir=project_folder)\n",
    "\n",
    "print(\" NLTK resources installed to:\", project_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00a01797-e659-4f45-9fc6-65f114d58331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated NLTK Path: ['C:/Project/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Clear existing paths and setting the project path\n",
    "nltk.data.path.clear()\n",
    "nltk_path = \"C:/Project/nltk_data\"\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "print(\"Updated NLTK Path:\", nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b22afef-91db-49e5-a410-e6b1272a980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Project\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Project\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Project\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    try:\n",
    "        # Converting to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Removing special characters and numbers\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "        # Tokenization using regex\n",
    "        tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "        # Removing stopwords and applying lemmatization\n",
    "        processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "        # Joins tokens back to a cleaned sentence\n",
    "        cleaned_text = \" \".join(processed_tokens)\n",
    "        return cleaned_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return text\n",
    "\n",
    "def preprocess_loaded_data(dataframes):\n",
    "    try:\n",
    "        for name, df in dataframes.items():\n",
    "            print(f\"Processing {name} data...\")\n",
    "\n",
    "            if 'content' in df.columns:\n",
    "                df['cleaned_text'] = df['content'].apply(preprocess_text)\n",
    "                print(f\"{name.capitalize()} data processed successfully!\\n\")\n",
    "            else:\n",
    "                print(f\"Skipping {name} as it has no 'content' column.\\n\")\n",
    "        \n",
    "        print(\"Preprocessing completed for all datasets.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a7708-344a-4b75-a86a-3840e188c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_loaded_data(dataframes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abd36b6a-84e5-475c-8735-a744acc0bc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Hello darling! I love talking to you every single day. Isn't it wonderful?\n",
      "Original Text: Hello darling! I love talking to you every single day. Isn't it wonderful?...\n",
      "Lowercased Text: hello darling! i love talking to you every single day. isn't it wonderful?...\n",
      "Cleaned Text (No Special Chars & Numbers): hello darling i love talking to you every single day isnt it wonderful...\n",
      "Tokens: ['hello', 'darling', 'i', 'love', 'talking', 'to', 'you', 'every', 'single', 'day', 'isnt', 'it', 'wonderful']...\n",
      "Processed Tokens (After Stopword Removal & Lemmatization): ['hello', 'darling', 'love', 'talking', 'every', 'single', 'day', 'isnt', 'wonderful']...\n",
      "Final Cleaned Text: hello darling love talking every single day isnt wonderful...\n",
      "\n",
      "Cleaned Text: hello darling love talking every single day isnt wonderful\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Hello darling! I love talking to you every single day. Isn't it wonderful?\"\n",
    "print(\"Original Text:\", sample_text)\n",
    "\n",
    "cleaned_text = preprocess_text(sample_text)\n",
    "print(\"Cleaned Text:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a156a6c1-d539-4226-ab57-fb59133afab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_polarity_train saved successfully as Parquet!\n",
      "amazon_polarity_test saved successfully as Parquet!\n",
      "go_emotions_train saved successfully as Parquet!\n",
      "go_emotions_test saved successfully as Parquet!\n",
      "go_emotions_validation saved successfully as Parquet!\n",
      "imdb_train saved successfully as Parquet!\n",
      "imdb_test saved successfully as Parquet!\n",
      "imdb_unsupervised saved successfully as Parquet!\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes.items():\n",
    "    df.to_parquet(f\"C:/Project/dataset_cache/{name}_preprocessed.parquet\")\n",
    "    print(f\"{name} saved successfully as Parquet!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbeea68-d9cc-4be0-81c0-3facd0490e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
